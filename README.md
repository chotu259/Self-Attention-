**Transformer**

I learned about transformer architecture and implemented the self-attention mechanism along with multi-head attention.
